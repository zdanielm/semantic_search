{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Data Analysis Project (Semantic Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data Into DB & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl  # Dataframe library for loading initial dataset\n",
    "from playhouse.postgres_ext import PostgresqlExtDatabase, fn  # Peewee ORM with extras\n",
    "\n",
    "from utilities.models import (  # ORM Classes\n",
    "    database_driver, # Database driver to be used (Uses `config.yaml`)\n",
    "    create_tables,\n",
    "    Patent, # Main class, holding the texts\n",
    "    arctic_noverlap,\n",
    "    arctic_recursive,\n",
    "    arctic_sliding,\n",
    "    minilm_noverlap,\n",
    "    minilm_recursive,\n",
    "    minilm_sliding,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.setup import load_config  # Loading Database config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test split of \"Nuclear Patents\", which is small enough for efficient handling, but large enough for more advanced methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patent_number</th><th>section</th><th>raw_text</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;062326139&quot;</td><td>&quot;abstract&quot;</td><td>&quot;An angular pumped and emitting…</td></tr><tr><td>&quot;059600497&quot;</td><td>&quot;abstract&quot;</td><td>&quot;The operator of a nuclear stea…</td></tr><tr><td>&quot;042499950&quot;</td><td>&quot;abstract&quot;</td><td>&quot;In a fast reactor constituted …</td></tr><tr><td>&quot;051606950&quot;</td><td>&quot;abstract&quot;</td><td>&quot;An apparatus and method of enh…</td></tr><tr><td>&quot;044477333&quot;</td><td>&quot;claims&quot;</td><td>&quot;1. A radiation-shielding trans…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌───────────────┬──────────┬─────────────────────────────────┐\n",
       "│ patent_number ┆ section  ┆ raw_text                        │\n",
       "│ ---           ┆ ---      ┆ ---                             │\n",
       "│ str           ┆ str      ┆ str                             │\n",
       "╞═══════════════╪══════════╪═════════════════════════════════╡\n",
       "│ 062326139     ┆ abstract ┆ An angular pumped and emitting… │\n",
       "│ 059600497     ┆ abstract ┆ The operator of a nuclear stea… │\n",
       "│ 042499950     ┆ abstract ┆ In a fast reactor constituted … │\n",
       "│ 051606950     ┆ abstract ┆ An apparatus and method of enh… │\n",
       "│ 044477333     ┆ claims   ┆ 1. A radiation-shielding trans… │\n",
       "└───────────────┴──────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads train and test split of \"Nuclear Patents\"\n",
    "# Small enough for efficient handling\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "train = pl.read_parquet('hf://datasets/arcee-ai/nuclear_patents/' + splits['train'])\n",
    "test = pl.read_parquet('hf://datasets/arcee-ai/nuclear_patents/' + splits['test'])\n",
    "\n",
    "# Combining train and test splits and filtering out nulls\n",
    "patent_data = pl.concat([train, test]).filter(\n",
    "    pl.col(\"patent_number\").is_not_null() & (pl.col(\"patent_number\").str.len_chars() > 0) &\n",
    "    pl.col(\"section\").is_not_null() & (pl.col(\"section\").str.len_chars() > 0) &\n",
    "    pl.col(\"raw_text\").is_not_null() & (pl.col(\"raw_text\").str.len_chars() > 0) &\n",
    "    (pl.col(\"raw_text\").str.len_chars() <= 5000)  # Filter for raw_text length <= 5000\n",
    ")\n",
    "\n",
    "patent_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the specified tables (Connection and ORM models are set up in `utilities/models.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_driver.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting the data from `patent_data` into `patents` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_patents(patent_data: pl.DataFrame) -> None:\n",
    "    with database_driver.atomic():\n",
    "        batch_size = 1000\n",
    "\n",
    "        patent_records = patent_data.to_dicts()\n",
    "\n",
    "        for i in range(0, len(patent_records), batch_size):\n",
    "            batch = patent_records[i:i + batch_size]\n",
    "\n",
    "            # Add search_vector field before inserting\n",
    "            for record in batch:\n",
    "                record[\"search_vector\"] = fn.to_tsvector('english', record[\"raw_text\"])\n",
    "\n",
    "            # Bulk insert batch\n",
    "            Patent.insert_many(batch).execute()\n",
    "\n",
    "            print(f\"Inserted records {i} to {min(i + batch_size, len(patent_records))}\")\n",
    "\n",
    "    print(f\"Insertion: Successful ({len(patent_records)} records created)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted records 0 to 1000\n",
      "Inserted records 1000 to 2000\n",
      "Inserted records 2000 to 3000\n",
      "Inserted records 3000 to 4000\n",
      "Inserted records 4000 to 5000\n",
      "Inserted records 5000 to 6000\n",
      "Inserted records 6000 to 6678\n",
      "Insertion: Successful (6678 records created)\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_patents(patent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading embedding models (MiniLM, Arctic Embed M) and chunkers (No overlap splitter, recursive splitter, sliding window splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowSplitter():\n",
    "    def __init__(self, window_size=400, step=100):\n",
    "        self.window_size = window_size\n",
    "        self.step = step\n",
    "\n",
    "    def split_text(self, text):\n",
    "        \"\"\"Implements sliding window chunking .\"\"\"\n",
    "        words = text.split() # Splits at whitespaces\n",
    "        chunks = [' '.join(words[i:i+self.window_size]) for i in range(0, len(words), self.step)]\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_lm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "arctic_embed = SentenceTransformer('Snowflake/snowflake-arctic-embed-m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_size_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 100,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_splitter = SlidingWindowSplitter(\n",
    "    window_size = 400,\n",
    "    step = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_chunks_and_embeddings(chunker, embedding_model, chunk_db):\n",
    "    patent_data = Patent.select()\n",
    "    batch_size = 1000  # Choose a batch size for bulk insert\n",
    "    for i in range(0, len(patent_data), batch_size):\n",
    "        batch = patent_data[i:i + batch_size]\n",
    "        chunk_data = []\n",
    "        chunks_to_embed = []\n",
    "\n",
    "        for patent in batch:\n",
    "            # Chunk the raw_text\n",
    "            chunks = chunker.split_text(patent.raw_text)\n",
    "            chunks_to_embed.extend(chunks)  # Collect chunks\n",
    "\n",
    "            # Prepare chunk_data entries\n",
    "            for chunk in chunks:\n",
    "                chunk_data.append({\n",
    "                    'patent_number': patent.id,\n",
    "                    'chunk_text': chunk,\n",
    "                    'embedding': None  # Placeholder for embedding\n",
    "                })\n",
    "\n",
    "        # Generate embeddings in bulk\n",
    "        embeddings = embedding_model.encode(chunks_to_embed)\n",
    "\n",
    "        # Now update chunk_data\n",
    "        for idx, embedding in enumerate(embeddings):\n",
    "            chunk_data[idx]['embedding'] = embedding.tolist()\n",
    "\n",
    "        # Insert chunks in bulk\n",
    "        with database_driver.atomic():\n",
    "            chunk_db.insert_many(chunk_data).execute()  # Bulk insert into the given chunk_db table\n",
    "        print(f\"Inserted chunks for patents {i} to {i + len(batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    fixed_size_splitter,\n",
    "    mini_lm,\n",
    "    minilm_noverlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    sentence_splitter,\n",
    "    mini_lm,\n",
    "    minilm_recursive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    window_splitter,\n",
    "    mini_lm,\n",
    "    minilm_sliding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    fixed_size_splitter,\n",
    "    arctic_embed,\n",
    "    arctic_noverlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    sentence_splitter,\n",
    "    arctic_embed,\n",
    "    arctic_recursive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    window_splitter,\n",
    "    arctic_embed,\n",
    "    arctic_sliding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(keyword, chunk_db, embedding_model, fetch_size=10):\n",
    "    embedding = embedding_model.encode(keyword)\n",
    "\n",
    "    results = chunk_db \\\n",
    "        .select(chunk_db.patent_number, chunk_db.chunk_text) \\\n",
    "        .where(chunk_db.embedding.cosine_distance(embedding) < 0.6) \\\n",
    "        .order_by(chunk_db.embedding.cosine_distance(embedding).asc()) \\\n",
    "        .limit(fetch_size)\n",
    "\n",
    "    return pl.DataFrame(list(results.dicts())) if results.exists() else pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(keyword, chunk_db, patent_db, embedding_model, fetch_size=10, lambda_weight=0.5):\n",
    "    embedding = embedding_model.encode(keyword)\n",
    "\n",
    "    # Semantic Search\n",
    "    semantic_results = chunk_db \\\n",
    "        .select(chunk_db.patent_number, chunk_db.chunk_text, \n",
    "                (1 - chunk_db.embedding.cosine_distance(embedding)).alias('semantic_score')) \\\n",
    "        .where(chunk_db.embedding.cosine_distance(embedding) < 0.6) \\\n",
    "        .order_by((1 - chunk_db.embedding.cosine_distance(embedding)).desc()) \\\n",
    "        .limit(100)\n",
    "\n",
    "    semantic_df = pl.DataFrame(list(semantic_results.dicts())) if semantic_results.exists() else pl.DataFrame()\n",
    "\n",
    "    # Keyword-based Full-Text Search\n",
    "    keyword_results = patent_db \\\n",
    "        .select(patent_db.id, patent_db.patent_number, patent_db.raw_text, \n",
    "                fn.ts_rank_cd(patent_db.search_vector, fn.to_tsquery(keyword)).alias('keyword_score')) \\\n",
    "        .where(patent_db.search_vector.match(keyword)) \\\n",
    "        .order_by(fn.ts_rank_cd(patent_db.search_vector, fn.to_tsquery(keyword)).desc()) \\\n",
    "        .limit(100)\n",
    "\n",
    "    keyword_df = pl.DataFrame(list(keyword_results.dicts())) if keyword_results.exists() else pl.DataFrame()\n",
    "\n",
    "    if keyword_df.is_empty() and semantic_df.is_empty():\n",
    "        return pl.DataFrame()  # No results\n",
    "\n",
    "    # Ensure both DataFrames have the same type for `id`\n",
    "    keyword_df = keyword_df.with_columns(pl.col(\"id\").cast(pl.Utf8))\n",
    "    semantic_df = semantic_df.with_columns(pl.col(\"patent_number\").cast(pl.Utf8))\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    keyword_df = keyword_df.select([\"id\", \"keyword_score\"])\n",
    "    semantic_df = semantic_df.select([\"patent_number\", \"semantic_score\"])\n",
    "\n",
    "    # Merge on 'id' from keyword_df and 'patent_number' from semantic_df\n",
    "    combined_df = keyword_df.join(semantic_df, left_on=\"id\", right_on=\"patent_number\", how=\"inner\").fill_null(0)\n",
    "\n",
    "    combined_df = combined_df.with_columns(\n",
    "        pl.col(\"semantic_score\").cast(pl.Float64),\n",
    "        pl.col(\"keyword_score\").cast(pl.Float64)\n",
    "    )\n",
    "\n",
    "    # Compute Final Hybrid Score\n",
    "    combined_df = combined_df.with_columns(\n",
    "        ((lambda_weight * combined_df[\"semantic_score\"]) + \n",
    "         ((1 - lambda_weight) * combined_df[\"keyword_score\"])).alias(\"final_score\")\n",
    "    )\n",
    "\n",
    "    # Get Top `patent_number`s Based on Hybrid Score\n",
    "    top_patent_ids = combined_df.sort(\"final_score\", descending=True)[\"id\"].head(fetch_size).to_list()\n",
    "\n",
    "    if not top_patent_ids:\n",
    "        return pl.DataFrame()\n",
    "\n",
    "    # Fetch Full Patent Data for the Top IDs\n",
    "    full_patent_data = patent_db.select(patent_db.patent_number, patent_db.section ,patent_db.raw_text).where(patent_db.id.in_(top_patent_ids))\n",
    "\n",
    "    return pl.DataFrame(list(full_patent_data.dicts()))  # Convert to Polars DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = [\n",
    "    \"nuclear stream\",\n",
    "    \"reactor\",\n",
    "    \"nuclear fusion\",\n",
    "    \"radiation-shielding\",\n",
    "    \"X-ray\",\n",
    "    \"irradiation\",\n",
    "    \"Si crystal\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patent_number</th><th>chunk_text</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>18524</td><td>&quot;incident to the X-ray detector…</td></tr><tr><td>13570</td><td>&quot;of the X-ray beam.&quot;</td></tr><tr><td>14460</td><td>&quot;passage of X-rays correspondin…</td></tr><tr><td>17716</td><td>&quot;rest. 13. An X-ray examination…</td></tr><tr><td>14589</td><td>&quot;1. An X-ray examination appara…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────────┬─────────────────────────────────┐\n",
       "│ patent_number ┆ chunk_text                      │\n",
       "│ ---           ┆ ---                             │\n",
       "│ i64           ┆ str                             │\n",
       "╞═══════════════╪═════════════════════════════════╡\n",
       "│ 18524         ┆ incident to the X-ray detector… │\n",
       "│ 13570         ┆ of the X-ray beam.              │\n",
       "│ 14460         ┆ passage of X-rays correspondin… │\n",
       "│ 17716         ┆ rest. 13. An X-ray examination… │\n",
       "│ 14589         ┆ 1. An X-ray examination appara… │\n",
       "└───────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = semantic_search(\n",
    "    keyword_list[4],\n",
    "    minilm_noverlap,\n",
    "    mini_lm\n",
    ")\n",
    "\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patent_number</th><th>section</th><th>raw_text</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;051596210&quot;</td><td>&quot;claims&quot;</td><td>&quot;1. An X-ray transmitting windo…</td></tr><tr><td>&quot;062755684&quot;</td><td>&quot;claims&quot;</td><td>&quot;1. An X-ray examination appara…</td></tr><tr><td>&quot;060944714&quot;</td><td>&quot;claims&quot;</td><td>&quot;1. X-ray concentrator comprisi…</td></tr><tr><td>&quot;046882421&quot;</td><td>&quot;summary&quot;</td><td>&quot;BACKGROUND OF THE INVENTION Th…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 3)\n",
       "┌───────────────┬─────────┬─────────────────────────────────┐\n",
       "│ patent_number ┆ section ┆ raw_text                        │\n",
       "│ ---           ┆ ---     ┆ ---                             │\n",
       "│ str           ┆ str     ┆ str                             │\n",
       "╞═══════════════╪═════════╪═════════════════════════════════╡\n",
       "│ 051596210     ┆ claims  ┆ 1. An X-ray transmitting windo… │\n",
       "│ 062755684     ┆ claims  ┆ 1. An X-ray examination appara… │\n",
       "│ 060944714     ┆ claims  ┆ 1. X-ray concentrator comprisi… │\n",
       "│ 046882421     ┆ summary ┆ BACKGROUND OF THE INVENTION Th… │\n",
       "└───────────────┴─────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = hybrid_search(\n",
    "    keyword_list[4],\n",
    "    minilm_noverlap,\n",
    "    Patent,\n",
    "    mini_lm\n",
    ")\n",
    "\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark Context Relevance Using TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark_retrieval import evaluate_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_for_keywords(model, embedding_model, search_function):\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for keyword in keyword_list:\n",
    "        # Get the relevance score for the current keyword\n",
    "        relevance_score = evaluate_retrieval(keyword, model, embedding_model, search_function)[\"context_relevance\"]\n",
    "        relevance_scores.append(relevance_score)\n",
    "    \n",
    "    # Compute the mean relevance for this model across all keywords\n",
    "    mean_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0\n",
    "    return mean_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n",
      "✅ In context_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In context_relevance, input context will be set to text .\n"
     ]
    }
   ],
   "source": [
    "minilm_noverlap_res = evaluate_model_for_keywords(minilm_noverlap, mini_lm, semantic_search)\n",
    "minilm_recursive_res = evaluate_model_for_keywords(minilm_recursive, mini_lm, semantic_search)\n",
    "minilm_sliding_res = evaluate_model_for_keywords(minilm_sliding, mini_lm, semantic_search)\n",
    "arctic_noverlap_res = evaluate_model_for_keywords(arctic_noverlap, arctic_embed, semantic_search)\n",
    "arctic_recursive_res = evaluate_model_for_keywords(arctic_recursive, arctic_embed, semantic_search)\n",
    "arctic_sliding_res = evaluate_model_for_keywords(arctic_sliding, arctic_embed, semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minilm_noverlap Result: 0.9047619047619048\n",
      "minilm_recursive Result: 1.0\n",
      "minilm_sliding Result: 1.0\n",
      "arctic_noverlap Result: 0.19047619047619047\n",
      "arctic_recursive Result: 0.19047619047619047\n",
      "arctic_sliding Result: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "print(\"minilm_noverlap Result:\", minilm_noverlap_res)\n",
    "print(\"minilm_recursive Result:\", minilm_recursive_res)\n",
    "print(\"minilm_sliding Result:\", minilm_sliding_res)\n",
    "print(\"arctic_noverlap Result:\", arctic_noverlap_res)\n",
    "print(\"arctic_recursive Result:\", arctic_recursive_res)\n",
    "print(\"arctic_sliding Result:\", arctic_sliding_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>elapsed_time</th><th>number_of_records</th><th>mean_context_relevance</th></tr><tr><td>str</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;minilm_noverlap&quot;</td><td>137.5</td><td>36849</td><td>0.904762</td></tr><tr><td>&quot;minilm_recursive&quot;</td><td>201.8</td><td>50475</td><td>1.0</td></tr><tr><td>&quot;minilm_sliding&quot;</td><td>95.1</td><td>25009</td><td>1.0</td></tr><tr><td>&quot;arctic_noverlap&quot;</td><td>341.7</td><td>36849</td><td>0.190476</td></tr><tr><td>&quot;arctic_recursive&quot;</td><td>511.7</td><td>50475</td><td>0.190476</td></tr><tr><td>&quot;arctic_sliding&quot;</td><td>330.8</td><td>25009</td><td>0.142857</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 4)\n",
       "┌──────────────────┬──────────────┬───────────────────┬────────────────────────┐\n",
       "│ model_name       ┆ elapsed_time ┆ number_of_records ┆ mean_context_relevance │\n",
       "│ ---              ┆ ---          ┆ ---               ┆ ---                    │\n",
       "│ str              ┆ f64          ┆ i64               ┆ f64                    │\n",
       "╞══════════════════╪══════════════╪═══════════════════╪════════════════════════╡\n",
       "│ minilm_noverlap  ┆ 137.5        ┆ 36849             ┆ 0.904762               │\n",
       "│ minilm_recursive ┆ 201.8        ┆ 50475             ┆ 1.0                    │\n",
       "│ minilm_sliding   ┆ 95.1         ┆ 25009             ┆ 1.0                    │\n",
       "│ arctic_noverlap  ┆ 341.7        ┆ 36849             ┆ 0.190476               │\n",
       "│ arctic_recursive ┆ 511.7        ┆ 50475             ┆ 0.190476               │\n",
       "│ arctic_sliding   ┆ 330.8        ┆ 25009             ┆ 0.142857               │\n",
       "└──────────────────┴──────────────┴───────────────────┴────────────────────────┘"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_data = pl.read_csv(\"./assets/benchmark.csv\")\n",
    "benchmark_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-15e8a72f64284d5fa153714d781cad3b.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-15e8a72f64284d5fa153714d781cad3b.vega-embed details,\n",
       "  #altair-viz-15e8a72f64284d5fa153714d781cad3b.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-15e8a72f64284d5fa153714d781cad3b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-15e8a72f64284d5fa153714d781cad3b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-15e8a72f64284d5fa153714d781cad3b\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-021585120445f9a8a0db3ab35442956b\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"model_name\", \"title\": \"Model Name\", \"type\": \"nominal\"}, \"y\": {\"field\": \"mean_context_relevance\", \"title\": \"Mean Context Relevance\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_16\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"title\": \"Model Performance Comparison by Mean Context Relevance (higher is better)\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-021585120445f9a8a0db3ab35442956b\": [{\"model_name\": \"minilm_noverlap\", \"elapsed_time\": 137.5, \"number_of_records\": 36849, \"mean_context_relevance\": 0.9047619047619048}, {\"model_name\": \"minilm_recursive\", \"elapsed_time\": 201.8, \"number_of_records\": 50475, \"mean_context_relevance\": 1.0}, {\"model_name\": \"minilm_sliding\", \"elapsed_time\": 95.1, \"number_of_records\": 25009, \"mean_context_relevance\": 1.0}, {\"model_name\": \"arctic_noverlap\", \"elapsed_time\": 341.7, \"number_of_records\": 36849, \"mean_context_relevance\": 0.19047619047619047}, {\"model_name\": \"arctic_recursive\", \"elapsed_time\": 511.7, \"number_of_records\": 50475, \"mean_context_relevance\": 0.19047619047619047}, {\"model_name\": \"arctic_sliding\", \"elapsed_time\": 330.8, \"number_of_records\": 25009, \"mean_context_relevance\": 0.14285714285714285}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the bar plot\n",
    "plot = benchmark_data.plot.bar(\n",
    "    x=\"model_name\", \n",
    "    y=\"mean_context_relevance\"\n",
    ")\n",
    "\n",
    "# Configure the plot with title and axis labels\n",
    "plot = plot.properties(\n",
    "    title=\"Model Performance Comparison by Mean Context Relevance (higher is better)\",\n",
    "    width=600\n",
    ").encode(\n",
    "    x=alt.X('model_name:N', title='Model Name'),  # Setting x-axis label\n",
    "    y=alt.Y('mean_context_relevance:Q', title='Mean Context Relevance')  # Setting y-axis label\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-987f4db3aa33420c8ca430dce930aee5.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-987f4db3aa33420c8ca430dce930aee5.vega-embed details,\n",
       "  #altair-viz-987f4db3aa33420c8ca430dce930aee5.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-987f4db3aa33420c8ca430dce930aee5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-987f4db3aa33420c8ca430dce930aee5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-987f4db3aa33420c8ca430dce930aee5\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-021585120445f9a8a0db3ab35442956b\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"model_name\", \"title\": \"Model Name\", \"type\": \"nominal\"}, \"y\": {\"field\": \"elapsed_time\", \"title\": \"Elapsed Time\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_13\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"title\": \"Model Performance Comparison by Elapsed Time (lower is better)\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-021585120445f9a8a0db3ab35442956b\": [{\"model_name\": \"minilm_noverlap\", \"elapsed_time\": 137.5, \"number_of_records\": 36849, \"mean_context_relevance\": 0.9047619047619048}, {\"model_name\": \"minilm_recursive\", \"elapsed_time\": 201.8, \"number_of_records\": 50475, \"mean_context_relevance\": 1.0}, {\"model_name\": \"minilm_sliding\", \"elapsed_time\": 95.1, \"number_of_records\": 25009, \"mean_context_relevance\": 1.0}, {\"model_name\": \"arctic_noverlap\", \"elapsed_time\": 341.7, \"number_of_records\": 36849, \"mean_context_relevance\": 0.19047619047619047}, {\"model_name\": \"arctic_recursive\", \"elapsed_time\": 511.7, \"number_of_records\": 50475, \"mean_context_relevance\": 0.19047619047619047}, {\"model_name\": \"arctic_sliding\", \"elapsed_time\": 330.8, \"number_of_records\": 25009, \"mean_context_relevance\": 0.14285714285714285}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the bar plot\n",
    "plot = benchmark_data.plot.bar(\n",
    "    x=\"model_name\", \n",
    "    y=\"elapsed_time\"\n",
    ")\n",
    "\n",
    "# Configure the plot with title and axis labels\n",
    "plot = plot.properties(\n",
    "    title=\"Model Performance Comparison by Elapsed Time (lower is better)\",\n",
    "    width=600\n",
    ").encode(\n",
    "    x=alt.X('model_name:N', title='Model Name'),  # Setting x-axis label\n",
    "    y=alt.Y('elapsed_time:Q', title='Elapsed Time')  # Setting y-axis label\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-80b9a1a8f2804e269a4529420b89dda2.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-80b9a1a8f2804e269a4529420b89dda2.vega-embed details,\n",
       "  #altair-viz-80b9a1a8f2804e269a4529420b89dda2.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-80b9a1a8f2804e269a4529420b89dda2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-80b9a1a8f2804e269a4529420b89dda2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-80b9a1a8f2804e269a4529420b89dda2\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-021585120445f9a8a0db3ab35442956b\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"model_name\", \"title\": \"Model Name\", \"type\": \"nominal\"}, \"y\": {\"field\": \"number_of_records\", \"title\": \"Number of DataBase Records\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_15\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"title\": \"Model Performance Comparison by Number of DataBase Records\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-021585120445f9a8a0db3ab35442956b\": [{\"model_name\": \"minilm_noverlap\", \"elapsed_time\": 137.5, \"number_of_records\": 36849, \"mean_context_relevance\": 0.9047619047619048}, {\"model_name\": \"minilm_recursive\", \"elapsed_time\": 201.8, \"number_of_records\": 50475, \"mean_context_relevance\": 1.0}, {\"model_name\": \"minilm_sliding\", \"elapsed_time\": 95.1, \"number_of_records\": 25009, \"mean_context_relevance\": 1.0}, {\"model_name\": \"arctic_noverlap\", \"elapsed_time\": 341.7, \"number_of_records\": 36849, \"mean_context_relevance\": 0.19047619047619047}, {\"model_name\": \"arctic_recursive\", \"elapsed_time\": 511.7, \"number_of_records\": 50475, \"mean_context_relevance\": 0.19047619047619047}, {\"model_name\": \"arctic_sliding\", \"elapsed_time\": 330.8, \"number_of_records\": 25009, \"mean_context_relevance\": 0.14285714285714285}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the bar plot\n",
    "plot = benchmark_data.plot.bar(\n",
    "    x=\"model_name\", \n",
    "    y=\"number_of_records\"\n",
    ")\n",
    "\n",
    "# Configure the plot with title and axis labels\n",
    "plot = plot.properties(\n",
    "    title=\"Model Performance Comparison by Number of DataBase Records\",\n",
    "    width=600\n",
    ").encode(\n",
    "    x=alt.X('model_name:N', title='Model Name'),  # Setting x-axis label\n",
    "    y=alt.Y('number_of_records:Q', title='Number of DataBase Records')  # Setting y-axis label\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
