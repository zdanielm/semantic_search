{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Data Analysis Project (Semantic Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data Into DB & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl  # Dataframe library for loading initial dataset\n",
    "from playhouse.postgres_ext import PostgresqlExtDatabase, fn  # Peewee ORM with extras\n",
    "\n",
    "from utilities.models import (  # ORM Classes\n",
    "    database_driver, # Database driver to be used (Uses `config.yaml`)\n",
    "    create_tables,\n",
    "    Patent, # Main class, holding the texts\n",
    "    arctic_noverlap,\n",
    "    arctic_recursive,\n",
    "    arctic_sliding,\n",
    "    minilm_noverlap,\n",
    "    minilm_recursive,\n",
    "    minilm_sliding,\n",
    ")\n",
    "from utilities.setup import load_config  # Loading Database config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test split of \"Nuclear Patents\", which is small enough for efficient handling, but large enough for more advanced methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patent_number</th><th>section</th><th>raw_text</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;062326139&quot;</td><td>&quot;abstract&quot;</td><td>&quot;An angular pumped and emitting…</td></tr><tr><td>&quot;059600497&quot;</td><td>&quot;abstract&quot;</td><td>&quot;The operator of a nuclear stea…</td></tr><tr><td>&quot;042499950&quot;</td><td>&quot;abstract&quot;</td><td>&quot;In a fast reactor constituted …</td></tr><tr><td>&quot;051606950&quot;</td><td>&quot;abstract&quot;</td><td>&quot;An apparatus and method of enh…</td></tr><tr><td>&quot;044477333&quot;</td><td>&quot;claims&quot;</td><td>&quot;1. A radiation-shielding trans…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌───────────────┬──────────┬─────────────────────────────────┐\n",
       "│ patent_number ┆ section  ┆ raw_text                        │\n",
       "│ ---           ┆ ---      ┆ ---                             │\n",
       "│ str           ┆ str      ┆ str                             │\n",
       "╞═══════════════╪══════════╪═════════════════════════════════╡\n",
       "│ 062326139     ┆ abstract ┆ An angular pumped and emitting… │\n",
       "│ 059600497     ┆ abstract ┆ The operator of a nuclear stea… │\n",
       "│ 042499950     ┆ abstract ┆ In a fast reactor constituted … │\n",
       "│ 051606950     ┆ abstract ┆ An apparatus and method of enh… │\n",
       "│ 044477333     ┆ claims   ┆ 1. A radiation-shielding trans… │\n",
       "└───────────────┴──────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads train and test split of \"Nuclear Patents\"\n",
    "# Small enough for efficient handling\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "train = pl.read_parquet('hf://datasets/arcee-ai/nuclear_patents/' + splits['train'])\n",
    "test = pl.read_parquet('hf://datasets/arcee-ai/nuclear_patents/' + splits['test'])\n",
    "\n",
    "# Combining train and test splits and filtering out nulls\n",
    "patent_data = pl.concat([train, test]).filter(\n",
    "    pl.col(\"patent_number\").is_not_null() & (pl.col(\"patent_number\").str.len_chars() > 0) &\n",
    "    pl.col(\"section\").is_not_null() & (pl.col(\"section\").str.len_chars() > 0) &\n",
    "    pl.col(\"raw_text\").is_not_null() & (pl.col(\"raw_text\").str.len_chars() > 0) &\n",
    "    (pl.col(\"raw_text\").str.len_chars() <= 5000)  # Filter for raw_text length <= 5000\n",
    ")\n",
    "\n",
    "patent_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the specified tables (Connection and ORM models are set up in `utilities/models.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_driver.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting the data from `patent_data` into `patents` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_patents(patent_data: pl.DataFrame) -> None:\n",
    "    with database_driver.atomic():\n",
    "        batch_size = 1000\n",
    "\n",
    "        patent_records = patent_data.to_dicts()\n",
    "\n",
    "        for i in range(0, len(patent_records), batch_size):\n",
    "            batch = patent_records[i:i + batch_size]\n",
    "\n",
    "            # Add search_vector field before inserting\n",
    "            for record in batch:\n",
    "                record[\"search_vector\"] = fn.to_tsvector('english', record[\"raw_text\"])\n",
    "\n",
    "            # Bulk insert batch\n",
    "            Patent.insert_many(batch).execute()\n",
    "\n",
    "            print(f\"Inserted records {i} to {min(i + batch_size, len(patent_records))}\")\n",
    "\n",
    "    print(f\"Insertion: Successful ({len(patent_records)} records created)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted records 0 to 1000\n",
      "Inserted records 1000 to 2000\n",
      "Inserted records 2000 to 3000\n",
      "Inserted records 3000 to 4000\n",
      "Inserted records 4000 to 5000\n",
      "Inserted records 5000 to 6000\n",
      "Inserted records 6000 to 6678\n",
      "Insertion: Successful (6678 records created)\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_patents(patent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading embedding models (MiniLM, Arctic Embed M) and chunkers (No overlap splitter, recursive splitter, sliding window splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowSplitter():\n",
    "    def __init__(self, window_size=400, step=100):\n",
    "        self.window_size = window_size\n",
    "        self.step = step\n",
    "\n",
    "    def split_text(self, text):\n",
    "        \"\"\"Implements sliding window chunking .\"\"\"\n",
    "        words = text.split() # Splits at whitespaces\n",
    "        chunks = [' '.join(words[i:i+self.window_size]) for i in range(0, len(words), self.step)]\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_lm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "arctic_embed = SentenceTransformer('Snowflake/snowflake-arctic-embed-m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_size_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=400,  # Number of characters per chunk\n",
    "    chunk_overlap=0  # No overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 100,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_splitter = SlidingWindowSplitter(\n",
    "    window_size = 400,\n",
    "    step = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_chunks_and_embeddings(chunker, embedding_model, chunk_db):\n",
    "    patent_data = Patent.select()  # Load all patents, you can add filters as needed\n",
    "    batch_size = 1000  # Choose a batch size for bulk insert\n",
    "    for i in range(0, len(patent_data), batch_size):\n",
    "        batch = patent_data[i:i + batch_size]\n",
    "        chunk_data = []\n",
    "        chunks_to_embed = []  # Store chunks to be embedded in bulk\n",
    "\n",
    "        for patent in batch:\n",
    "            # Chunk the raw_text into smaller parts\n",
    "            chunks = chunker.split_text(patent.raw_text)\n",
    "            chunks_to_embed.extend(chunks)  # Collect all chunks in this batch\n",
    "\n",
    "            # Prepare chunk_data entries (without embeddings for now)\n",
    "            for chunk in chunks:\n",
    "                chunk_data.append({\n",
    "                    'patent_number': patent.id,\n",
    "                    'chunk_text': chunk,\n",
    "                    'embedding': None  # Placeholder for embedding, to be updated later\n",
    "                })\n",
    "\n",
    "        # Generate embeddings in bulk for the chunks\n",
    "        embeddings = embedding_model.encode(chunks_to_embed)\n",
    "\n",
    "        # Now update chunk_data with the generated embeddings\n",
    "        for idx, embedding in enumerate(embeddings):\n",
    "            chunk_data[idx]['embedding'] = embedding.tolist()  # Assuming embedding is a numpy array or list\n",
    "\n",
    "        # Insert chunks in bulk\n",
    "        with database_driver.atomic():\n",
    "            chunk_db.insert_many(chunk_data).execute()  # Bulk insert into the given chunk_db table\n",
    "        print(f\"Inserted chunks for patents {i} to {i + len(batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    fixed_size_splitter,\n",
    "    mini_lm,\n",
    "    minilm_noverlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    sentence_splitter,\n",
    "    mini_lm,\n",
    "    minilm_recursive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    window_splitter,\n",
    "    mini_lm,\n",
    "    minilm_sliding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    fixed_size_splitter,\n",
    "    arctic_embed,\n",
    "    arctic_noverlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    sentence_splitter,\n",
    "    arctic_embed,\n",
    "    arctic_recursive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunks for patents 0 to 1000\n",
      "Inserted chunks for patents 1000 to 2000\n",
      "Inserted chunks for patents 2000 to 3000\n",
      "Inserted chunks for patents 3000 to 4000\n",
      "Inserted chunks for patents 4000 to 5000\n",
      "Inserted chunks for patents 5000 to 6000\n",
      "Inserted chunks for patents 6000 to 6678\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_chunks_and_embeddings(\n",
    "    window_splitter,\n",
    "    arctic_embed,\n",
    "    arctic_sliding\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
