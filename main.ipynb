{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Data Analysis Project (Semantic Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data Into DB & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl  # Dataframe library for loading initial dataset\n",
    "from playhouse.postgres_ext import PostgresqlExtDatabase, fn  # Peewee ORM with extras\n",
    "\n",
    "from utilities.models import (  # ORM Classes\n",
    "    database_driver, # Database driver to be used (Uses `config.yaml`)\n",
    "    create_tables,\n",
    "    Patent, # Main class, holding the texts\n",
    "    arctic_noverlap,\n",
    "    arctic_recursive,\n",
    "    arctic_sliding,\n",
    "    minilm_noverlap,\n",
    "    minilm_recursive,\n",
    "    minilm_sliding,\n",
    ")\n",
    "from utilities.setup import load_config  # Loading Database config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test split of \"Nuclear Patents\", which is small enough for efficient handling, but large enough for more advanced methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patent_number</th><th>section</th><th>raw_text</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;062326139&quot;</td><td>&quot;abstract&quot;</td><td>&quot;An angular pumped and emitting…</td></tr><tr><td>&quot;059600497&quot;</td><td>&quot;abstract&quot;</td><td>&quot;The operator of a nuclear stea…</td></tr><tr><td>&quot;042499950&quot;</td><td>&quot;abstract&quot;</td><td>&quot;In a fast reactor constituted …</td></tr><tr><td>&quot;051606950&quot;</td><td>&quot;abstract&quot;</td><td>&quot;An apparatus and method of enh…</td></tr><tr><td>&quot;044477333&quot;</td><td>&quot;claims&quot;</td><td>&quot;1. A radiation-shielding trans…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌───────────────┬──────────┬─────────────────────────────────┐\n",
       "│ patent_number ┆ section  ┆ raw_text                        │\n",
       "│ ---           ┆ ---      ┆ ---                             │\n",
       "│ str           ┆ str      ┆ str                             │\n",
       "╞═══════════════╪══════════╪═════════════════════════════════╡\n",
       "│ 062326139     ┆ abstract ┆ An angular pumped and emitting… │\n",
       "│ 059600497     ┆ abstract ┆ The operator of a nuclear stea… │\n",
       "│ 042499950     ┆ abstract ┆ In a fast reactor constituted … │\n",
       "│ 051606950     ┆ abstract ┆ An apparatus and method of enh… │\n",
       "│ 044477333     ┆ claims   ┆ 1. A radiation-shielding trans… │\n",
       "└───────────────┴──────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads train and test split of \"Nuclear Patents\"\n",
    "# Small enough for efficient handling\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "train = pl.read_parquet('hf://datasets/arcee-ai/nuclear_patents/' + splits['train'])\n",
    "test = pl.read_parquet('hf://datasets/arcee-ai/nuclear_patents/' + splits['test'])\n",
    "\n",
    "# Combining train and test splits and filtering out nulls\n",
    "patent_data = pl.concat([train, test]).filter(\n",
    "    pl.col(\"patent_number\").is_not_null() & (pl.col(\"patent_number\").str.len_chars() > 0) &\n",
    "    pl.col(\"section\").is_not_null() & (pl.col(\"section\").str.len_chars() > 0) &\n",
    "    pl.col(\"raw_text\").is_not_null() & (pl.col(\"raw_text\").str.len_chars() > 0) &\n",
    "    (pl.col(\"raw_text\").str.len_chars() <= 5000)  # Add filter for raw_text length <= 5000\n",
    ")\n",
    "\n",
    "patent_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the specified tables (Connection and ORM models are set up in `utilities/models.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "Connection already opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdatabase_driver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\projects\\semantic_search\\.venv\\Lib\\site-packages\\peewee.py:3259\u001b[39m, in \u001b[36mDatabase.connect\u001b[39m\u001b[34m(self, reuse_if_open)\u001b[39m\n\u001b[32m   3257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reuse_if_open:\n\u001b[32m   3258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(\u001b[33m'\u001b[39m\u001b[33mConnection already opened.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3261\u001b[39m \u001b[38;5;28mself\u001b[39m._state.reset()\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m __exception_wrapper__:\n",
      "\u001b[31mOperationalError\u001b[39m: Connection already opened."
     ]
    }
   ],
   "source": [
    "database_driver.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting the data from `patent_data` into `patents` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_patents(patent_data: pl.DataFrame) -> None:\n",
    "    with database_driver.atomic():\n",
    "        batch_size = 1000\n",
    "\n",
    "        patent_records = patent_data.to_dicts()\n",
    "\n",
    "        for i in range(0, len(patent_records), batch_size):\n",
    "            batch = patent_records[i:i + batch_size]\n",
    "\n",
    "            # Add search_vector field before inserting\n",
    "            for record in batch:\n",
    "                record[\"search_vector\"] = fn.to_tsvector('english', record[\"raw_text\"])\n",
    "\n",
    "            # Bulk insert batch\n",
    "            Patent.insert_many(batch).execute()\n",
    "\n",
    "            print(f\"Inserted records {i} to {min(i + batch_size, len(patent_records))}\")\n",
    "\n",
    "    print(f\"Insertion: Successful ({len(patent_records)} records created)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted records 0 to 1000\n",
      "Inserted records 1000 to 2000\n",
      "Inserted records 2000 to 3000\n",
      "Inserted records 3000 to 4000\n",
      "Inserted records 4000 to 5000\n",
      "Inserted records 5000 to 6000\n",
      "Inserted records 6000 to 6678\n",
      "Insertion: Successful (6678 records created)\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_patents(patent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading embedding models (MiniLM, Arctic Embed M) and chunkers (No overlap splitter, recursive splitter, sliding window splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_lm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "arctic_embed = SentenceTransformer('Snowflake/snowflake-arctic-embed-m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_size_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=400,  # Number of characters per chunk\n",
    "    chunk_overlap=0  # No overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 100,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunking(text, window_size=400, step=100):\n",
    "    \"\"\"Implements sliding window chunking with LangChain.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+window_size]) for i in range(0, len(words), step)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert(patent_data: pl.DataFrame, splitter) -> None:\n",
    "    with database_driver.atomic():\n",
    "        batch_size = 1000\n",
    "\n",
    "        patent_records = patent_data.to_dicts()\n",
    "\n",
    "        for i in range(0, len(patent_records), batch_size):\n",
    "            batch = patent_records[i:i + batch_size]\n",
    "\n",
    "            # Add search_vector field before inserting\n",
    "            for record in batch:\n",
    "                record[\"search_vector\"] = fn.to_tsvector('english', record[\"raw_text\"])\n",
    "\n",
    "            # Bulk insert batch\n",
    "            Patent.insert_many(batch).execute()\n",
    "\n",
    "            print(f\"Inserted records {i} to {min(i + batch_size, len(patent_records))}\")\n",
    "\n",
    "    print(f\"Insertion: Successful ({len(patent_records)} records created)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
